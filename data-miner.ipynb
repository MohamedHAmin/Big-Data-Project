{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697dbc2c-1dca-4629-9c32-7df596e8b35a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfd6870-e9b5-465f-b10b-22cb9265b6ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_url = 'local[*]'\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '24g')\n",
    "spark = SparkSession.builder.master(master_url).appName(\"data-miner\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7276f93-ed33-4b6b-91c3-6f6db58f6097",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec978f0f-c78b-43ec-85c4-4accc8541ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = './matches-1k.json'\n",
    "league_df = spark.read.option(\"inferTimestamp\", \"false\").option(\"mode\", \"DROPMALFORMED\").json(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8515b5c6-9353-4b40-9617-b822226d02c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- gameCreation: long (nullable = true)\n",
      " |-- gameDuration: long (nullable = true)\n",
      " |-- gameId: long (nullable = true)\n",
      " |-- gameMode: string (nullable = true)\n",
      " |-- gameType: string (nullable = true)\n",
      " |-- gameVersion: string (nullable = true)\n",
      " |-- mapId: long (nullable = true)\n",
      " |-- participantIdentities: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- participantId: long (nullable = true)\n",
      " |    |    |-- player: struct (nullable = true)\n",
      " |    |    |    |-- accountId: string (nullable = true)\n",
      " |    |    |    |-- currentAccountId: string (nullable = true)\n",
      " |    |    |    |-- currentPlatformId: string (nullable = true)\n",
      " |    |    |    |-- matchHistoryUri: string (nullable = true)\n",
      " |    |    |    |-- platformId: string (nullable = true)\n",
      " |    |    |    |-- profileIcon: long (nullable = true)\n",
      " |    |    |    |-- summonerId: string (nullable = true)\n",
      " |    |    |    |-- summonerName: string (nullable = true)\n",
      " |-- participants: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- championId: string (nullable = true)\n",
      " |    |    |-- participantId: long (nullable = true)\n",
      " |    |    |-- spell1Id: string (nullable = true)\n",
      " |    |    |-- spell2Id: string (nullable = true)\n",
      " |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |-- assists: long (nullable = true)\n",
      " |    |    |    |-- champLevel: long (nullable = true)\n",
      " |    |    |    |-- combatPlayerScore: long (nullable = true)\n",
      " |    |    |    |-- damageDealtToObjectives: long (nullable = true)\n",
      " |    |    |    |-- damageDealtToTurrets: long (nullable = true)\n",
      " |    |    |    |-- damageSelfMitigated: long (nullable = true)\n",
      " |    |    |    |-- deaths: long (nullable = true)\n",
      " |    |    |    |-- doubleKills: long (nullable = true)\n",
      " |    |    |    |-- firstBloodAssist: boolean (nullable = true)\n",
      " |    |    |    |-- firstBloodKill: boolean (nullable = true)\n",
      " |    |    |    |-- firstInhibitorAssist: boolean (nullable = true)\n",
      " |    |    |    |-- firstInhibitorKill: boolean (nullable = true)\n",
      " |    |    |    |-- firstTowerAssist: boolean (nullable = true)\n",
      " |    |    |    |-- firstTowerKill: boolean (nullable = true)\n",
      " |    |    |    |-- goldEarned: long (nullable = true)\n",
      " |    |    |    |-- goldSpent: long (nullable = true)\n",
      " |    |    |    |-- inhibitorKills: long (nullable = true)\n",
      " |    |    |    |-- item0: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item1: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item2: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item3: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item4: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item5: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |    |    |-- FlatArmorMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatCritChanceMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatHPRegenMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMPPoolMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMagicDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatMovementSpeedMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatPhysicalDamageMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- FlatSpellBlockMod: long (nullable = true)\n",
      " |    |    |    |    |    |-- PercentAttackSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentLifeStealMod: double (nullable = true)\n",
      " |    |    |    |    |    |-- PercentMovementSpeedMod: double (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- item6: struct (nullable = true)\n",
      " |    |    |    |    |-- from: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- gold: struct (nullable = true)\n",
      " |    |    |    |    |    |-- base: long (nullable = true)\n",
      " |    |    |    |    |    |-- purchasable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- sell: long (nullable = true)\n",
      " |    |    |    |    |    |-- total: long (nullable = true)\n",
      " |    |    |    |    |-- into: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- killingSprees: long (nullable = true)\n",
      " |    |    |    |-- kills: long (nullable = true)\n",
      " |    |    |    |-- largestCriticalStrike: long (nullable = true)\n",
      " |    |    |    |-- largestKillingSpree: long (nullable = true)\n",
      " |    |    |    |-- largestMultiKill: long (nullable = true)\n",
      " |    |    |    |-- longestTimeSpentLiving: long (nullable = true)\n",
      " |    |    |    |-- magicDamageDealt: long (nullable = true)\n",
      " |    |    |    |-- magicDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |    |-- magicalDamageTaken: long (nullable = true)\n",
      " |    |    |    |-- neutralMinionsKilled: long (nullable = true)\n",
      " |    |    |    |-- neutralMinionsKilledEnemyJungle: long (nullable = true)\n",
      " |    |    |    |-- neutralMinionsKilledTeamJungle: long (nullable = true)\n",
      " |    |    |    |-- objectivePlayerScore: long (nullable = true)\n",
      " |    |    |    |-- participantId: long (nullable = true)\n",
      " |    |    |    |-- pentaKills: long (nullable = true)\n",
      " |    |    |    |-- perk0: string (nullable = true)\n",
      " |    |    |    |-- perk0Var1: long (nullable = true)\n",
      " |    |    |    |-- perk0Var2: long (nullable = true)\n",
      " |    |    |    |-- perk0Var3: long (nullable = true)\n",
      " |    |    |    |-- perk1: string (nullable = true)\n",
      " |    |    |    |-- perk1Var1: long (nullable = true)\n",
      " |    |    |    |-- perk1Var2: long (nullable = true)\n",
      " |    |    |    |-- perk1Var3: long (nullable = true)\n",
      " |    |    |    |-- perk2: string (nullable = true)\n",
      " |    |    |    |-- perk2Var1: long (nullable = true)\n",
      " |    |    |    |-- perk2Var2: long (nullable = true)\n",
      " |    |    |    |-- perk2Var3: long (nullable = true)\n",
      " |    |    |    |-- perk3: string (nullable = true)\n",
      " |    |    |    |-- perk3Var1: long (nullable = true)\n",
      " |    |    |    |-- perk3Var2: long (nullable = true)\n",
      " |    |    |    |-- perk3Var3: long (nullable = true)\n",
      " |    |    |    |-- perk4: string (nullable = true)\n",
      " |    |    |    |-- perk4Var1: long (nullable = true)\n",
      " |    |    |    |-- perk4Var2: long (nullable = true)\n",
      " |    |    |    |-- perk4Var3: long (nullable = true)\n",
      " |    |    |    |-- perk5: string (nullable = true)\n",
      " |    |    |    |-- perk5Var1: long (nullable = true)\n",
      " |    |    |    |-- perk5Var2: long (nullable = true)\n",
      " |    |    |    |-- perk5Var3: long (nullable = true)\n",
      " |    |    |    |-- perkPrimaryStyle: string (nullable = true)\n",
      " |    |    |    |-- perkSubStyle: string (nullable = true)\n",
      " |    |    |    |-- physicalDamageDealt: long (nullable = true)\n",
      " |    |    |    |-- physicalDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |    |-- physicalDamageTaken: long (nullable = true)\n",
      " |    |    |    |-- playerScore0: long (nullable = true)\n",
      " |    |    |    |-- playerScore1: long (nullable = true)\n",
      " |    |    |    |-- playerScore2: long (nullable = true)\n",
      " |    |    |    |-- playerScore3: long (nullable = true)\n",
      " |    |    |    |-- playerScore4: long (nullable = true)\n",
      " |    |    |    |-- playerScore5: long (nullable = true)\n",
      " |    |    |    |-- playerScore6: long (nullable = true)\n",
      " |    |    |    |-- playerScore7: long (nullable = true)\n",
      " |    |    |    |-- playerScore8: long (nullable = true)\n",
      " |    |    |    |-- playerScore9: long (nullable = true)\n",
      " |    |    |    |-- quadraKills: long (nullable = true)\n",
      " |    |    |    |-- sightWardsBoughtInGame: long (nullable = true)\n",
      " |    |    |    |-- statPerk0: string (nullable = true)\n",
      " |    |    |    |-- statPerk1: string (nullable = true)\n",
      " |    |    |    |-- statPerk2: string (nullable = true)\n",
      " |    |    |    |-- timeCCingOthers: long (nullable = true)\n",
      " |    |    |    |-- totalDamageDealt: long (nullable = true)\n",
      " |    |    |    |-- totalDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |    |-- totalDamageTaken: long (nullable = true)\n",
      " |    |    |    |-- totalHeal: long (nullable = true)\n",
      " |    |    |    |-- totalMinionsKilled: long (nullable = true)\n",
      " |    |    |    |-- totalPlayerScore: long (nullable = true)\n",
      " |    |    |    |-- totalScoreRank: long (nullable = true)\n",
      " |    |    |    |-- totalTimeCrowdControlDealt: long (nullable = true)\n",
      " |    |    |    |-- totalUnitsHealed: long (nullable = true)\n",
      " |    |    |    |-- tripleKills: long (nullable = true)\n",
      " |    |    |    |-- trueDamageDealt: long (nullable = true)\n",
      " |    |    |    |-- trueDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |    |-- trueDamageTaken: long (nullable = true)\n",
      " |    |    |    |-- turretKills: long (nullable = true)\n",
      " |    |    |    |-- unrealKills: long (nullable = true)\n",
      " |    |    |    |-- visionScore: long (nullable = true)\n",
      " |    |    |    |-- visionWardsBoughtInGame: long (nullable = true)\n",
      " |    |    |    |-- wardsKilled: long (nullable = true)\n",
      " |    |    |    |-- wardsPlaced: long (nullable = true)\n",
      " |    |    |    |-- win: boolean (nullable = true)\n",
      " |    |    |-- teamId: string (nullable = true)\n",
      " |    |    |-- timeline: struct (nullable = true)\n",
      " |    |    |    |-- creepsPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- csDiffPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- damageTakenDiffPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- damageTakenPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- goldPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- lane: string (nullable = true)\n",
      " |    |    |    |-- participantId: long (nullable = true)\n",
      " |    |    |    |-- role: string (nullable = true)\n",
      " |    |    |    |-- xpDiffPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |    |    |    |-- xpPerMinDeltas: struct (nullable = true)\n",
      " |    |    |    |    |-- 0-10: double (nullable = true)\n",
      " |    |    |    |    |-- 10-20: double (nullable = true)\n",
      " |    |    |    |    |-- 20-30: double (nullable = true)\n",
      " |    |    |    |    |-- 30-end: double (nullable = true)\n",
      " |-- platformId: string (nullable = true)\n",
      " |-- queueId: long (nullable = true)\n",
      " |-- seasonId: long (nullable = true)\n",
      " |-- teams: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- bans: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- championId: string (nullable = true)\n",
      " |    |    |    |    |-- pickTurn: long (nullable = true)\n",
      " |    |    |-- baronKills: long (nullable = true)\n",
      " |    |    |-- dominionVictoryScore: long (nullable = true)\n",
      " |    |    |-- dragonKills: long (nullable = true)\n",
      " |    |    |-- firstBaron: boolean (nullable = true)\n",
      " |    |    |-- firstBlood: boolean (nullable = true)\n",
      " |    |    |-- firstDragon: boolean (nullable = true)\n",
      " |    |    |-- firstInhibitor: boolean (nullable = true)\n",
      " |    |    |-- firstRiftHerald: boolean (nullable = true)\n",
      " |    |    |-- firstTower: boolean (nullable = true)\n",
      " |    |    |-- inhibitorKills: long (nullable = true)\n",
      " |    |    |-- riftHeraldKills: long (nullable = true)\n",
      " |    |    |-- teamId: string (nullable = true)\n",
      " |    |    |-- towerKills: long (nullable = true)\n",
      " |    |    |-- vilemawKills: long (nullable = true)\n",
      " |    |    |-- win: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "league_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4672413-eee5-49ef-a18f-61cd375e5606",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- participants: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- championId: string (nullable = true)\n",
      " |    |    |-- spell1Id: string (nullable = true)\n",
      " |    |    |-- spell2Id: string (nullable = true)\n",
      " |    |    |-- stats: struct (nullable = true)\n",
      " |    |    |    |-- assists: long (nullable = true)\n",
      " |    |    |    |-- deaths: long (nullable = true)\n",
      " |    |    |    |-- goldEarned: long (nullable = true)\n",
      " |    |    |    |-- item0: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item1: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item2: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item3: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item4: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item5: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- item6: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- kills: long (nullable = true)\n",
      " |    |    |    |-- perk0: string (nullable = true)\n",
      " |    |    |    |-- perk1: string (nullable = true)\n",
      " |    |    |    |-- perk2: string (nullable = true)\n",
      " |    |    |    |-- perk3: string (nullable = true)\n",
      " |    |    |    |-- perk4: string (nullable = true)\n",
      " |    |    |    |-- perk5: string (nullable = true)\n",
      " |    |    |    |-- totalDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |    |-- totalDamageTaken: long (nullable = true)\n",
      " |    |    |    |-- totalHeal: long (nullable = true)\n",
      " |    |    |    |-- win: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleansed_league_df = league_df\n",
    "cleansed_league_df = cleansed_league_df.drop('_id', 'gameCreation', 'gameDuration', 'gameId', 'gameMode', 'gameType', 'gameVersion', 'mapId', 'participantIdentities', 'platformId', 'queueId', 'seasonId', 'teams') # can use teams\n",
    "cleansed_league_df = cleansed_league_df.withColumn(\"participants\",\n",
    "    F.transform(\n",
    "        cleansed_league_df[\"participants\"],\n",
    "        lambda x: x.withField(\"stats\", \n",
    "                                x[\"stats\"].dropFields('champLevel', 'combatPlayerScore',\n",
    "                                    'damageDealtToObjectives', 'damageDealtToTurrets', 'damageSelfMitigated', 'doubleKills',\n",
    "                                    'firstBloodAssist', 'firstBloodKill', 'firstInhibitorAssist', 'firstInhibitorKill', \n",
    "                                    'firstTowerAssist', 'firstTowerKill', 'goldSpent', 'inhibitorKills',\n",
    "                                    'killingSprees', 'largestCriticalStrike', 'largestKillingSpree',\n",
    "                                    'largestMultiKill', 'longestTimeSpentLiving', 'magicDamageDealt', \n",
    "                                    'magicDamageDealtToChampions', 'magicalDamageTaken', 'neutralMinionsKilled', \n",
    "                                    'neutralMinionsKilledEnemyJungle', 'neutralMinionsKilledTeamJungle', \n",
    "                                    'objectivePlayerScore', 'participantId', 'pentaKills', \n",
    "                                    'perk0Var1', 'perk0Var2', 'perk0Var3', 'perk1Var1', 'perk1Var2', \n",
    "                                    'perk1Var3', 'perk2Var1', 'perk2Var2', 'perk2Var3', \n",
    "                                    'perk3Var1', 'perk3Var2', 'perk3Var3', 'perk4Var1', 'perk4Var2', \n",
    "                                    'perk4Var3', 'perk5Var1', 'perk5Var2', 'perk5Var3', 'perkPrimaryStyle', \n",
    "                                    'perkSubStyle', 'physicalDamageDealt', 'physicalDamageDealtToChampions', \n",
    "                                    'physicalDamageTaken', 'playerScore0', 'playerScore1', 'playerScore2', \n",
    "                                    'playerScore3', 'playerScore4', 'playerScore5', 'playerScore6', 'playerScore7', \n",
    "                                    'playerScore8', 'playerScore9', 'quadraKills', 'sightWardsBoughtInGame', \n",
    "                                    'statPerk0', 'statPerk1', 'statPerk2', 'timeCCingOthers', 'totalDamageDealt', \n",
    "                                    'totalMinionsKilled', \n",
    "                                    'totalPlayerScore', 'totalScoreRank', 'totalTimeCrowdControlDealt', 'totalUnitsHealed', \n",
    "                                    'tripleKills', 'trueDamageDealt', 'trueDamageDealtToChampions', 'trueDamageTaken', \n",
    "                                    'turretKills', 'unrealKills', 'visionScore', 'visionWardsBoughtInGame', 'wardsKilled', \n",
    "                                    'wardsPlaced')\n",
    "                            )\n",
    "    )\n",
    ")\n",
    "cleansed_league_df = cleansed_league_df.withColumn(\"participants\", \n",
    "    F.transform(\n",
    "        cleansed_league_df[\"participants\"],\n",
    "        lambda x: x.dropFields('timeline', 'participantId', 'killingSprees', 'kills', 'largestCriticalStrike', 'largestKillingSpree', 'largestMultiKill', 'longestTimeSpentLiving', 'magicDamageDealt', 'magicDamageDealtToChampions', 'magicalDamageTaken', 'neutralMinionsKilled', 'neutralMinionsKilledEnemyJungle', 'neutralMinionsKilledTeamJungle', 'objectivePlayerScore', 'participantId', 'pentaKills', 'perk0', 'perk0Var1', 'perk0Var2', 'perk0Var3', 'perk1', 'perk1Var1', 'perk1Var2', 'perk1Var3', 'perk2', 'perk2Var1', 'perk2Var2', 'perk2Var3', 'perk3', 'perk3Var1', 'perk3Var2', 'perk3Var3', 'perk4', 'perk4Var1', 'perk4Var2', 'perk4Var3', 'perk5', 'perk5Var1', 'perk5Var2', 'perk5Var3', 'perkPrimaryStyle', 'perkSubStyle', 'physicalDamageDealt', 'physicalDamageDealtToChampions', 'physicalDamageTaken', 'playerScore0', 'playerScore1', 'playerScore2', 'playerScore3', 'playerScore4', 'playerScore5', 'playerScore6', 'playerScore7', 'playerScore8', 'playerScore9', 'quadraKills', 'sightWardsBoughtInGame', 'statPerk0', 'statPerk1', 'statPerk2', 'timeCCingOthers', 'totalDamageDealt', 'totalDamageDealtToChampions', 'totalDamageTaken', 'totalHeal', 'totalMinionsKilled', 'totalPlayerScore', 'totalScoreRank', 'totalTimeCrowdControlDealt', 'totalUnitsHealed', 'tripleKills', 'trueDamageDealt', 'trueDamageDealtToChampions', 'trueDamageTaken', 'turretKills', 'unrealKills', 'visionScore', 'visionWardsBoughtInGame', 'wardsKilled', 'wardsPlaced', 'teamId') # can use timeline later\n",
    "    )\n",
    ")\n",
    "for i in range(7):\n",
    "    item = \"item\" + str(i)\n",
    "    cleansed_league_df = cleansed_league_df.withColumn(\"participants\", \n",
    "        F.transform(\n",
    "            cleansed_league_df[\"participants\"],\n",
    "            lambda x: x.withField(\"stats\" , \n",
    "                x[\"stats\"].withField(item, \n",
    "                    x[\"stats\"][item].dropFields('from', 'gold', 'into', 'stats', 'tags')\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "cleansed_league_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6241ce6-d1c7-4ecb-a596-c6ac230147e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- participants_exploded: struct (nullable = true)\n",
      " |    |-- championId: string (nullable = true)\n",
      " |    |-- spell1Id: string (nullable = true)\n",
      " |    |-- spell2Id: string (nullable = true)\n",
      " |    |-- stats: struct (nullable = true)\n",
      " |    |    |-- assists: long (nullable = true)\n",
      " |    |    |-- deaths: long (nullable = true)\n",
      " |    |    |-- goldEarned: long (nullable = true)\n",
      " |    |    |-- item0: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item1: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item2: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item3: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item4: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item5: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- item6: struct (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- kills: long (nullable = true)\n",
      " |    |    |-- perk0: string (nullable = true)\n",
      " |    |    |-- perk1: string (nullable = true)\n",
      " |    |    |-- perk2: string (nullable = true)\n",
      " |    |    |-- perk3: string (nullable = true)\n",
      " |    |    |-- perk4: string (nullable = true)\n",
      " |    |    |-- perk5: string (nullable = true)\n",
      " |    |    |-- totalDamageDealtToChampions: long (nullable = true)\n",
      " |    |    |-- totalDamageTaken: long (nullable = true)\n",
      " |    |    |-- totalHeal: long (nullable = true)\n",
      " |    |    |-- win: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explode_df(nested_df):\n",
    "    new_df = nested_df\n",
    "    for column in nested_df.columns:\n",
    "        if cleansed_league_df.schema[column].dataType.typeName() == 'array':\n",
    "            new_df = nested_df.selectExpr(\"*\", f\"explode({column}) as {column}_exploded\").drop(column)\n",
    "    return new_df\n",
    "\n",
    "exploded_league_df = explode_df(cleansed_league_df)\n",
    "exploded_league_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(value,mean,std_dev):\n",
    "    if value <= mean - std_dev:\n",
    "        return \"Low\"\n",
    "    elif value <= mean + std_dev:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2353 2.8521478546419137\n",
      "5.2222 4.0593441573114655\n",
      "7.9182 5.581919431741583\n",
      "10306.0865 3502.812180939095\n",
      "15328.9367 9464.465698347085\n",
      "20153.444 10121.023860858335\n",
      "5607.1283 5725.508000607954\n",
      "15328.9367 9464.465698347085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'High'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanDeaths = exploded_league_df.agg(F.mean(\"participants_exploded.stats.deaths\")).collect()[0][0]\n",
    "StdDevDeaths = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.deaths\")).collect()[0][0]\n",
    "\n",
    "MeanKills = exploded_league_df.agg(F.mean(\"participants_exploded.stats.kills\")).collect()[0][0]\n",
    "StdDevKills = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.kills\")).collect()[0][0]\n",
    "\n",
    "MeanAssists = exploded_league_df.agg(F.mean(\"participants_exploded.stats.assists\")).collect()[0][0]\n",
    "StdDevAssists = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.assists\")).collect()[0][0]\n",
    "\n",
    "MeanGoldEarned = exploded_league_df.agg(F.mean(\"participants_exploded.stats.goldEarned\")).collect()[0][0]\n",
    "StdDevGoldEarned = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.goldEarned\")).collect()[0][0]\n",
    "\n",
    "MeanTotalDamageDealtToChampions = exploded_league_df.agg(F.mean(\"participants_exploded.stats.totalDamageDealtToChampions\")).collect()[0][0]\n",
    "StdDevTotalDamageDealtToChampions = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.totalDamageDealtToChampions\")).collect()[0][0]\n",
    "\n",
    "MeanTotalDamageTaken = exploded_league_df.agg(F.mean(\"participants_exploded.stats.totalDamageTaken\")).collect()[0][0]\n",
    "StdDevTotalDamageTaken = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.totalDamageTaken\")).collect()[0][0]\n",
    "\n",
    "MeanTotalHeal = exploded_league_df.agg(F.mean(\"participants_exploded.stats.totalHeal\")).collect()[0][0]\n",
    "StdDevTotalHeal = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.totalHeal\")).collect()[0][0]\n",
    "\n",
    "MeantotalDamageDealtToChampions = exploded_league_df.agg(F.mean(\"participants_exploded.stats.totalDamageDealtToChampions\")).collect()[0][0]\n",
    "StdDevtotalDamageDealtToChampions = exploded_league_df.agg(F.stddev(\"participants_exploded.stats.totalDamageDealtToChampions\")).collect()[0][0]\n",
    "\n",
    "print(MeanDeaths,StdDevDeaths)\n",
    "print(MeanKills,StdDevKills)\n",
    "print(MeanAssists,StdDevAssists)\n",
    "print(MeanGoldEarned,StdDevGoldEarned)\n",
    "print(MeanTotalDamageDealtToChampions,StdDevTotalDamageDealtToChampions)\n",
    "print(MeanTotalDamageTaken,StdDevTotalDamageTaken)\n",
    "print(MeanTotalHeal,StdDevTotalHeal)\n",
    "print(MeantotalDamageDealtToChampions,StdDevtotalDamageDealtToChampions)\n",
    "\n",
    "categorize(10,MeanDeaths,StdDevDeaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd742ac-557f-4d51-8aca-f7db1e0113a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "league_rdd = exploded_league_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5b3fd1-70b0-4c55-a72b-1b1d028547fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def itemize(record):\n",
    "    items = []\n",
    "    participant = record.participants_exploded\n",
    "    items.append(\"Champion = \"+ participant.championId)\n",
    "    items.append(\"Spell = \"+ participant.spell1Id)\n",
    "    items.append(\"Spell = \"+ participant.spell2Id)\n",
    "    stats = participant['stats']\n",
    "    items.append(\"Win = \" + str(stats.win))\n",
    "    for i in range(7):\n",
    "        item = stats['item' + str(i)]\n",
    "        item_name = item['name'] if item else None\n",
    "        if item_name: items.append(\"Item = \" + item_name)\n",
    "    for i in range(6):\n",
    "        perk = stats['perk' + str(i)]\n",
    "        perk_name = item['name'] if perk else None\n",
    "        if perk_name: items.append(\"Perk = \" + perk_name)\n",
    "    return list(set(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a4111a-be22-4070-84d9-18edcd37f08c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "items_rdd = league_rdd.map(itemize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28152f8-6114-40cc-9d55-31edaa9c3e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "minSupport = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6021d5bd-7dba-4abb-9cb8-3fa553fc166d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o381.trainFPGrowthModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 49.0 failed 1 times, most recent failure: Lost task 11.0 in stage 49.0 (TID 248) (LAPTOP-UROEI7PJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\r\n\tat org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:216)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainFPGrowthModel(PythonMLLibAPI.scala:573)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m FPGrowth\u001b[39m.\u001b[39;49mtrain(items_rdd, minSupport\u001b[39m=\u001b[39;49mminSupport)\n\u001b[0;32m      2\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfreqItemsets()\n\u001b[0;32m      3\u001b[0m result_sorted \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39msortBy(\u001b[39mlambda\u001b[39;00m x : (\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mitems), \u001b[39m-\u001b[39mx\u001b[39m.\u001b[39mfreq))\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\mllib\\fpm.py:101\u001b[0m, in \u001b[0;36mFPGrowth.train\u001b[1;34m(cls, data, minSupport, numPartitions)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[0;32m     82\u001b[0m     \u001b[39mcls\u001b[39m, data: RDD[List[T]], minSupport: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m, numPartitions: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     83\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFPGrowthModel\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    Computes an FP-Growth model that contains frequent itemsets.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m        (default: -1)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     model \u001b[39m=\u001b[39m callMLlibFunc(\u001b[39m\"\u001b[39;49m\u001b[39mtrainFPGrowthModel\u001b[39;49m\u001b[39m\"\u001b[39;49m, data, \u001b[39mfloat\u001b[39;49m(minSupport), \u001b[39mint\u001b[39;49m(numPartitions))\n\u001b[0;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m FPGrowthModel(model)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\mllib\\common.py:139\u001b[0m, in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m api \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonMLLibAPI(), name)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m callJavaFunc(sc, api, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\mllib\\common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m _java2py(sc, func(\u001b[39m*\u001b[39;49mjava_args))\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o381.trainFPGrowthModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 49.0 failed 1 times, most recent failure: Lost task 11.0 in stage 49.0 (TID 248) (LAPTOP-UROEI7PJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 21 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1266)\r\n\tat org.apache.spark.mllib.fpm.FPGrowth.run(FPGrowth.scala:216)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainFPGrowthModel(PythonMLLibAPI.scala:573)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "model = FPGrowth.train(items_rdd, minSupport=minSupport)\n",
    "result = model.freqItemsets()\n",
    "result_sorted = result.sortBy(lambda x : (-len(x.items), -x.freq))\n",
    "fi = result_sorted.collect()\n",
    "for i in fi:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a8b44a-22f2-4e3d-9026-13fb2a140616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m min_support \u001b[39m=\u001b[39m \u001b[39m0.025\u001b[39m\n\u001b[0;32m      5\u001b[0m item_counts \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mflatMap(\u001b[39mlambda\u001b[39;00m transaction: [(item, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m transaction])\u001b[39m.\u001b[39mreduceByKey(\u001b[39mlambda\u001b[39;00m a, b: a \u001b[39m+\u001b[39m b)\n\u001b[1;32m----> 6\u001b[0m total_count \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mcount()\n\u001b[0;32m      8\u001b[0m sup \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(min_support \u001b[39m*\u001b[39m total_count)\n\u001b[0;32m      9\u001b[0m freq_items \u001b[39m=\u001b[39m item_counts\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m sup)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2297\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m   2277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2278\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2272\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   2252\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[0;32m   2271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2272\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2273\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[0;32m   2274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2025\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[0;32m   2022\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2024\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2025\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m   2026\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "data = items_rdd\n",
    "min_support = 0.025\n",
    "\n",
    "item_counts = data.flatMap(lambda transaction: [(item, 1) for item in transaction]).reduceByKey(lambda a, b: a + b)\n",
    "total_count = data.count()\n",
    "\n",
    "sup = int(min_support * total_count)\n",
    "freq_items = item_counts.filter(lambda x: x[1] >= sup)\n",
    "rules = []\n",
    "k = 2\n",
    "while freq_items.count() > 0:\n",
    "    rules.append(freq_items.collect())\n",
    "    candidate_counts = data.flatMap(lambda transaction: [(pair, 1) for pair in combinations(transaction, k)]).reduceByKey(lambda a, b: a + b)\n",
    "    freq_items = candidate_counts.filter(lambda x: x[1] >= sup).map(lambda x: (x[0], x[1]))\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0717d106-de75-4cfa-87f9-b2bacd2f4553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 3.0 failed 1 times, most recent failure: Lost task 12.0 in stage 3.0 (TID 51) (LAPTOP-UROEI7PJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m min_support \u001b[39m=\u001b[39m minSupport\n\u001b[1;32m----> 2\u001b[0m total_count \u001b[39m=\u001b[39m items_rdd\u001b[39m.\u001b[39;49mcount()\n\u001b[0;32m      3\u001b[0m sup \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(total_count \u001b[39m*\u001b[39m min_support)\n\u001b[0;32m      5\u001b[0m broadcasted_items \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mbroadcast(items_rdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: \u001b[39mset\u001b[39m(x))\u001b[39m.\u001b[39mcollect())\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2297\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m   2277\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2278\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2272\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[NumberOrArray]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNumberOrArray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   2252\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[0;32m   2271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2272\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2273\u001b[0m         \u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd\n\u001b[0;32m   2274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2025\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[0;32m   2022\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2024\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2025\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m   2026\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Ahmed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 3.0 failed 1 times, most recent failure: Lost task 12.0 in stage 3.0 (TID 51) (LAPTOP-UROEI7PJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.base/java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "min_support = minSupport\n",
    "total_count = items_rdd.count()\n",
    "sup = int(total_count * min_support)\n",
    "\n",
    "broadcasted_items = sc.broadcast(items_rdd.map(lambda x: set(x)).collect())\n",
    "def sup_filter(x):\n",
    "    x_sup = len([1 for t in broadcasted_items.value if x.issubset(t)])\n",
    "    if x_sup >= sup:\n",
    "        return x, x_sup\n",
    "    return None\n",
    "\n",
    "rules = []\n",
    "\n",
    "k = 1\n",
    "ck = items_rdd.flatMap(lambda x: set(x)).distinct().collect()\n",
    "ck = [{x} for x in ck]\n",
    "\n",
    "while len(ck) > 0:\n",
    "    fk = sc.parallelize(ck).map(sup_filter).filter(lambda x: x is not None).collect()\n",
    "    if len(fk): rules.append(fk)\n",
    "    k += 1\n",
    "    f_k_items = [item for item in map(lambda x: x[0], fk)]\n",
    "    ck = [i1 | i2 for i, i1 in enumerate(f_k_items) for i2 in f_k_items[i + 1:] if list(i1)[:k - 2] == list(i2)[:k - 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7254c67-19b2-4e01-8c51-42f6d74dc2ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'Perk = Oracle Lens', 'Item = Oracle Lens', 'Win = True', 'Item = Boots of Mobility', 'Spell = Flash', 'Spell = Ignite'}, 287), ({'Item = Control Ward', 'Perk = Oracle Lens', 'Item = Oracle Lens', 'Win = True', 'Spell = Flash', 'Spell = Ignite'}, 300), ({'Perk = Oracle Lens', 'Item = Oracle Lens', 'Win = False', 'Item = Enchantment: Runic Echoes', 'Spell = Flash', 'Spell = Smite'}, 263), ({\"Item = Sorcerer's Shoes\", 'Perk = Oracle Lens', 'Item = Oracle Lens', 'Item = Enchantment: Runic Echoes', 'Spell = Flash', 'Spell = Smite'}, 405), ({'Perk = Oracle Lens', 'Item = Oracle Lens', 'Win = False', 'Item = Boots of Mobility', 'Spell = Flash', 'Spell = Ignite'}, 286), ({'Item = Control Ward', 'Perk = Oracle Lens', 'Item = Oracle Lens', 'Win = False', 'Spell = Flash', 'Spell = Ignite'}, 419), ({'Item = Control Ward', 'Perk = Oracle Lens', 'Item = Oracle Lens', 'Item = Boots of Mobility', 'Spell = Flash', 'Spell = Ignite'}, 282), ({'Spell = Teleport', 'Win = True', 'Item = Warding Totem (Trinket)', 'Spell = Flash', 'Perk = Warding Totem (Trinket)', 'Item = Ninja Tabi'}, 377), ({'Win = True', 'Item = Infinity Edge', 'Perk = Farsight Alteration', 'Spell = Flash', 'Item = Farsight Alteration', \"Item = Berserker's Greaves\"}, 293), ({'Item = Enchantment: Warrior', 'Item = Warding Totem (Trinket)', 'Champion = Lee Sin', 'Spell = Flash', 'Spell = Smite', 'Perk = Warding Totem (Trinket)'}, 277), ({'Win = False', 'Item = Infinity Edge', 'Perk = Farsight Alteration', 'Spell = Flash', 'Item = Farsight Alteration', \"Item = Berserker's Greaves\"}, 281), ({'Spell = Teleport', 'Item = Warding Totem (Trinket)', 'Spell = Flash', 'Item = Black Cleaver', 'Perk = Warding Totem (Trinket)', 'Item = Ninja Tabi'}, 270)]\n"
     ]
    }
   ],
   "source": [
    "print(rules[5])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-miner",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
